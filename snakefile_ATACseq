# Snakefile for ATAC-seq analysis pipeline with RNA-seq alignment and filtering steps

import pandas as pd

# Configuration
configfile: "ref/config.yaml"

# Load sample information from a table
samples_df = pd.read_csv(config["samples_table"])

# Extract sample information
SAMPLES = samples_df['sample_id'].tolist()

# Define output directories
RESULT_DIR = "results"
FASTQC_DIR = f"{RESULT_DIR}/fastqc"
FASTP_DIR = f"{RESULT_DIR}/fastp"
ALIGN_DIR = f"{RESULT_DIR}/aligned"
TMP_DIR = f"{RESULT_DIR}/tmp"
FILTERED_DIR = f"{RESULT_DIR}/filtered"
DEDUP_DIR = f"{RESULT_DIR}/dedup"
BLACKLIST_FILTERED_DIR = f"{RESULT_DIR}/blacklist_filtered"
PEAKS_DIR = f"{RESULT_DIR}/peaks"
QC_DIR = f"{RESULT_DIR}/qc"

# Define final output files
rule all:
    input:
        # FastQC reports
        expand(f"{FASTQC_DIR}/{{sample}}_R1_001_fastqc.html", sample=SAMPLES),
        expand(f"{FASTQC_DIR}/{{sample}}_R2_001_fastqc.html", sample=SAMPLES),
        
        # Fastp reports and trimmed reads
        expand(f"{FASTP_DIR}/{{sample}}.html", sample=SAMPLES),
        expand(f"{FASTP_DIR}/{{sample}}.json", sample=SAMPLES),
        
        # Aligned SAM files
        expand(f"{ALIGN_DIR}/{{sample}}.sam", sample=SAMPLES),
        
        # Filtered BAM files
        expand(f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam", sample=SAMPLES),
        expand(f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam.bai", sample=SAMPLES),
        
        # Deduplicated BAM files
        expand(f"{DEDUP_DIR}/{{sample}}.dedup.bam", sample=SAMPLES),
        expand(f"{DEDUP_DIR}/{{sample}}.dedup.metrics.txt", sample=SAMPLES),
        
        # Blacklist filtered BAM files
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam", sample=SAMPLES),
        expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai", sample=SAMPLES),
        
        # Peak calling results (with blacklist filtering)
        expand(f"{PEAKS_DIR}/{{sample}}_peaks.narrowPeak", sample=SAMPLES),
        
        # Blacklist filtering statistics
        f"{QC_DIR}/blacklist_filtering_stats.txt"

# FastQC on raw reads
rule fastqc:
    input:
        r1 = "data/{sample}_R1_001.fastq.gz",
        r2 = "data/{sample}_R2_001.fastq.gz"
    output:
        r1_html = f"{FASTQC_DIR}/{{sample}}_R1_001_fastqc.html",
        r2_html = f"{FASTQC_DIR}/{{sample}}_R2_001_fastqc.html"
    params:
        outdir = FASTQC_DIR
    threads: 20
    log:
        "logs/fastqc/{sample}.log"
    shell:
        """
        mkdir -p {params.outdir}
        fastqc -t {threads} -o {params.outdir} {input.r1} {input.r2} > {log} 2>&1
        """

# Fastp for read trimming and quality filtering
rule fastp:
    input:
        r1 = "data/{sample}_R1_001.fastq.gz",
        r2 = "data/{sample}_R2_001.fastq.gz"
    output:
        r1 = f"{FASTP_DIR}/{{sample}}_R1.trimmed.fastq.gz",
        r2 = f"{FASTP_DIR}/{{sample}}_R2.trimmed.fastq.gz",
        html = f"{FASTP_DIR}/{{sample}}.html",
        json = f"{FASTP_DIR}/{{sample}}.json"
    threads: 20
    log:
        "logs/fastp/{sample}.log"
    shell:
        """
        mkdir -p {FASTP_DIR}
        fastp --in1 {input.r1} --in2 {input.r2} \
              --out1 {output.r1} --out2 {output.r2} \
              --thread {threads} \
              --html {output.html} \
              --json {output.json} \
              --detect_adapter_for_pe --trim_poly_g\
              --length_required 30 -p --cut_front --cut_tail --cut_window_size 4 --cut_mean_quality 20 > {log} 2>&1
        """

# REPLACED: Align reads with HISAT2 (from RNA-seq pipeline) instead of Bowtie2
rule hisat2_align:
    input:
        r1 = f"{FASTP_DIR}/{{sample}}_R1.trimmed.fastq.gz",
        r2 = f"{FASTP_DIR}/{{sample}}_R2.trimmed.fastq.gz"
    output:
        sam = f"{ALIGN_DIR}/{{sample}}.sam",
        summary = f"{ALIGN_DIR}/{{sample}}.sam.summary"
    log:
        "logs/hisat2/{sample}.log"
    params:
        index = config["hisat2_index"]  # Make sure to update config.yaml with hisat2_index
    threads: 20
    shell:
        """
        mkdir -p {ALIGN_DIR} logs/hisat2
        hisat2 -x {params.index} -1 {input.r1} -2 {input.r2} \
               -S {output.sam} \
               --summary-file {output.summary} \
               --no-spliced-alignment --add-chrname \
               -p {threads} \
               -q --phred33 -X 3000 -I 0 --no-discordant --no-mixed \
               &> {log}
        """

# REPLACED: Filter, sort, index BAM and generate flagstat (from RNA-seq pipeline)
rule samtools_sort_filter_index:
    input:
        f"{ALIGN_DIR}/{{sample}}.sam"
    output:
        bam = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam",
        bai = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam.bai",
        flagstat = f"{FILTERED_DIR}/{{sample}}_summary.txt"
    log:
        "logs/samtools/{sample}.log"
    threads: 20
    shell:
        """
        mkdir -p {FILTERED_DIR} logs/samtools {TMP_DIR}
        samtools view -@ {threads} -f 0x2 -F 0x100 -hS {input} | grep "NH:i:1\\|^@" > {TMP_DIR}/tmp_{wildcards.sample}.sam
        samtools view -@ {threads} -bhS {TMP_DIR}/tmp_{wildcards.sample}.sam | \
        samtools sort -@ {threads} -O bam -o {output.bam}
        samtools index -@ {threads} {output.bam} {output.bai}
        samtools flagstat {output.bam} > {output.flagstat}
        rm {TMP_DIR}/tmp_{wildcards.sample}.sam
        """

# Remove duplicates with Picard
rule remove_duplicates:
    input:
        filtered_bam = f"{FILTERED_DIR}/{{sample}}.sorted.filtered.bam"
    output:
        dedup_bam = f"{DEDUP_DIR}/{{sample}}.dedup.bam",
        metrics = f"{DEDUP_DIR}/{{sample}}.dedup.metrics.txt"
    threads: 4
    log:
        "logs/dedup/{sample}.log"
    shell:
        """
        mkdir -p {DEDUP_DIR}
        java -jar ref/picard.jar MarkDuplicates \
               INPUT={input.filtered_bam} \
               OUTPUT={output.dedup_bam} \
               METRICS_FILE={output.metrics} \
               REMOVE_DUPLICATES=true \
               ASSUME_SORTED=true \
               VALIDATION_STRINGENCY=LENIENT \
               TMP_DIR=tmp 2> {log}
        samtools index {output.dedup_bam}
        """

# Filter against blacklist regions
# Filter against blacklist regions
rule filter_blacklist:
    priority: 10
    input:
        bam = f"{DEDUP_DIR}/{{sample}}.dedup.bam",
        blacklist = config["blacklist"]
    output:
        filtered_bam = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam",
        filtered_bai = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam.bai",
        excluded_reads = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.blacklisted.bam"
    params:
        temp_bedpe = f"{TMP_DIR}/{{sample}}.fragments.bedpe",
        temp_fragment_bed = f"{TMP_DIR}/{{sample}}.fragments.bed",
        temp_blacklist_fragments = f"{TMP_DIR}/{{sample}}.blacklisted.fragments.bed",
        temp_blacklist_ids = f"{TMP_DIR}/{{sample}}.blacklisted.ids.txt",
        temp_namesorted_bam = f"{TMP_DIR}/{{sample}}.namesorted.bam",
        temp_filtered_bam = f"{TMP_DIR}/{{sample}}.filtered.bam",
        temp_excluded_bam = f"{TMP_DIR}/{{sample}}.excluded.bam"
    threads: 8
    log:
        "logs/blacklist_filter/{sample}.log"
    shell:
        """
        mkdir -p {BLACKLIST_FILTERED_DIR} {TMP_DIR}
        
        # Convert BAM to BEDPE format
        # This creates a BEDPE file with paired-end information
        samtools sort -n -@ {threads} {input.bam} | \
        bedtools bamtobed -bedpe -i stdin > {params.temp_bedpe} 2> {log}
        
        # Convert BEDPE to fragment BED (one entry per fragment)
        # Format: chrom, fragment_start, fragment_end, read_name, score, strand
        # We'll extract the fragment coordinates (minimum start, maximum end) 
        # and keep the read name for later filtering
        awk 'BEGIN {{OFS="\\t"}} {{if ($1==$4) print $1, ($2<$5?$2:$5), ($3>$6?$3:$6), $7, ".", ($9=="+"?"+":"-")}}' \
        {params.temp_bedpe} > {params.temp_fragment_bed} 2>> {log}
        
        # Find fragments that intersect with blacklisted regions
        bedtools intersect -a {params.temp_fragment_bed} -b {input.blacklist} -wa > {params.temp_blacklist_fragments} 2>> {log}
        
        # Extract read IDs from blacklisted fragments
        cut -f4 {params.temp_blacklist_fragments} | sort | uniq > {params.temp_blacklist_ids} 2>> {log}
        
        # Sort BAM by read name for paired-end processing (if not already name-sorted)
        samtools sort -n -@ {threads} -o {params.temp_namesorted_bam} {input.bam} 2>> {log}
        
        # Create properly paired BAMs - one with fragments that don't overlap blacklist
        
        # Filter out fragments overlapping blacklisted regions
        samtools view -@ {threads} -b -N ^{params.temp_blacklist_ids} \
            {params.temp_namesorted_bam} > {params.temp_filtered_bam} 2>> {log}
            
        # Extract fragments overlapping blacklisted regions
        samtools view -@ {threads} -b -N {params.temp_blacklist_ids} \
            {params.temp_namesorted_bam} > {params.temp_excluded_bam} 2>> {log}
            
        # Sort filtered BAM (non-blacklisted fragments) by coordinate for final output
        samtools sort -@ {threads} -o {output.filtered_bam} {params.temp_filtered_bam} 2>> {log}
        
        # Sort excluded BAM (blacklisted fragments) by coordinate for QC
        samtools sort -@ {threads} -o {output.excluded_reads} {params.temp_excluded_bam} 2>> {log}
        
        # Index the filtered BAM
        samtools index -@ {threads} {output.filtered_bam} {output.filtered_bai} 2>> {log}
        
        # Clean up temporary files
        rm -f {params.temp_bedpe} {params.temp_fragment_bed} {params.temp_blacklist_fragments} \
            {params.temp_blacklist_ids} {params.temp_namesorted_bam} {params.temp_filtered_bam} \
            {params.temp_excluded_bam}
        
        # Report stats
        echo "Blacklist filtering completed for {wildcards.sample}" >> {log}
        echo "$(wc -l < {params.temp_blacklist_fragments}) fragments overlap blacklisted regions" >> {log}
        echo "$(wc -l < {params.temp_blacklist_ids}) unique fragment IDs overlapping blacklisted regions" >> {log}
        echo "$(samtools view -c {output.excluded_reads}) total reads in excluded fragments" >> {log}
        echo "$(samtools view -c {output.filtered_bam}) total reads in filtered output" >> {log}
        """
        
# Call peaks with MACS2 (without input control) - now using blacklist filtered BAM
rule call_peaks:
    input:
        treatment = f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam"
    output:
        peaks = f"{PEAKS_DIR}/{{sample}}_peaks.narrowPeak"
    params:
        outdir = PEAKS_DIR,
        name = "{sample}"
    conda:
        "envs/macs2.yaml"
    log:
        "logs/macs2/{sample}.log"
    shell:
        """
        mkdir -p {params.outdir}
        macs2 callpeak \
              -t {input.treatment} \
              -f BAMPE \
              -g hs \
              --outdir {params.outdir} \
              -n {params.name} \
              --nomodel \
              --shift -75 \
              --extsize 150 \
              -q 0.05 > {log} 2>&1
        """
# Generate blacklist filtering statistics
rule blacklist_stats:
    input:
        original_bams = expand(f"{DEDUP_DIR}/{{sample}}.dedup.bam", sample=SAMPLES),
        filtered_bams = expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.nobl.bam", sample=SAMPLES),
        excluded_bams = expand(f"{BLACKLIST_FILTERED_DIR}/{{sample}}.blacklisted.bam", sample=SAMPLES)
    output:
        stats = f"{QC_DIR}/blacklist_filtering_stats.txt"
    params:
        samples = SAMPLES  # Pass sample names to the script
    threads: 1
    log:
        "logs/blacklist_stats/summary.log"
    script:
        "ref/blacklist-stats-script.py"